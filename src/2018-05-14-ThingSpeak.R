# created by Philip Orlando
# Sustainable Atmospheres Research Lab
# 2018-05-15
# automating data collection from Thingspeak

# load the necessary packages
if (!require(pacman)) {
  install.packages("pacman")
  library(pacman)
}

p_load(readr
       #,ggplot2
       ,plyr
       ,dplyr
       ,tidyr
       ,stringr
       ,magrittr
       ,rgeos
       ,rgdal
       ,sp
       ,leaflet
       ,sf
       ,raster
       ,mapview
       ,tidycensus
       ,tidyverse
       ,RPostgres
       ,RColorBrewer
       ,classInt
       ,htmltools
       ,scales
       ,htmlwidgets
       ,rPython
       ,devtools
       ,httr
       ,jsonlite
)


# geography projection
wgs_84 <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs "

# Sacramento, CA, UTM 10S, meters
epsg_26911 <- "+proj=utm +zone=10 +ellps=GRS80 +datum=NAD83 +units=m +no_defs"

# Oregon North NAD83 HARN meters
epsg_2838 <- "+proj=lcc +lat_1=46 +lat_2=44.33333333333334 +lat_0=43.66666666666666 +lon_0=-120.5 +x_0=2500000 +y_0=0 +ellps=GRS80 +units=m +no_defs "

# calling our purpleair json webscrape function to generate a list of ALL sensors
# python.load("./purpleair_id_key.py") # this unexpectedly crashes when running python within RStudio
# run from bash instead...

# reading in shapefiles for the entire US
urban_areas <- readOGR(dsn = "./data/tigerline/tl_2017_us_uac10.shp")

# filtering out only Portland, OR shapefiles
pdx <- subset(urban_areas, str_detect(NAME10, "Portland, OR"))
pdx <- spTransform(pdx, CRSobj = CRS(epsg_2838))
pdx <- st_as_sf(pdx)

# reading our scraped data in
pa <- read.csv("./data/pa_id_key/pa_id_key.txt"
               ,stringsAsFactors = FALSE
               ,header = TRUE)

# converting to simple features class
pa_sf <- st_as_sf(pa
                  ,coords = c("Lon", "Lat")
                  ,crs = wgs_84
                  ,na.fail = FALSE)

# transforming to NAD83 Oregon North to match our urban area data...
pa_sf <- st_transform(pa_sf, crs = st_crs(pdx))

# subsetting purpleair sensors that are contained within our urban area
pa_sf <- pa_sf[pdx, ]

# create a custom url generating function for each row
thingspeak_collect <- function(row, start_date, end_date) {
  
  # primary api id and key pairs
  primary_id <- row$THINGSPEAK_PRIMARY_ID
  primary_key <- row$THINGSPEAK_PRIMARY_ID_READ_KEY
  
  # secondary api id and key pairs
  secondary_id <- row$THINGSPEAK_SECONDARY_ID
  secondary_key <- row$THINGSPEAK_SECONDARY_ID_READ_KEY
  
  # primary url to pull from api
  primary_url <- paste0("https://api.thingspeak.com/channels/"
                        ,primary_id
                        ,"/feeds.json?api_key="
                        ,primary_key
                        ,"&start="
                        ,start_date
                        ,"%2000:00:00&end="
                        ,end_date
                        ,"%2000:00:00")
  
  # secondary url to pull from api
  secondary_url <- paste0("https://api.thingspeak.com/channels/"
                        ,secondary_id
                        ,"/feeds.json?api_key="
                        ,secondary_key
                        ,"&start="
                        ,start_date
                        ,"%2000:00:00&end="
                        ,end_date
                        ,"%2000:00:00")
  
  
  # this needs exception handling!
  primary_request <- fromJSON(primary_url)
  secondary_request <- fromJSON(secondary_url)
  
  # channel A field names
  primary_fields_a <- c("created_at"
                        ,"entry _id"
                        ,"pm1_0_atm"
                        ,"pm2_5_atm"
                        ,"pm10_0_atm"
                        ,"uptime_min"
                        ,"rssi_wifi_strength"
                        ,"temp_f"
                        ,"humidity"
                        ,"pm2_5_cf_1")
  
  secondary_fields_a <- c("created_at"
                          ,"entry_id"
                          ,"p_0_3_um"
                          ,"p_0_5_um"
                          ,"p_1_0_um"
                          ,"p_2_5_um"
                          ,"p_5_0_um"
                          ,"p_10_0_um"
                          ,"p1_0_cf_1"
                          ,"p10_0_cf_1")
  
  #channel B field names
  primary_fields_b <- c("created_at"
                        ,"entry_id"
                        ,"pm1_0_atm"
                        ,"pm2_5_atm"
                        ,"pm10_0_atm"
                        ,"free_heap_memory"
                        ,"analog_input"
                        ,"sensor_firmware_pressure"
                        ,"not_used"
                        ,"pm2_5_cf_1")
  
  secondary_fields_b <- c("created_at"
                          ,"entry_id"
                          ,"p_0_3_um"
                          ,"p_0_5_um"
                          ,"p_1_0_um"
                          ,"p_2_5_um"
                          ,"p_5_0_um"
                          ,"p_10_0_um"
                          ,"pm1_0_cf_1"
                          ,"pm10_0_cf_1")
  
  # A and B sensors provide different fields!
  if (!is.na(row$ParentID)) {
    
    # assign A field names
    primary_df <- primary_request$feeds
    colnames(primary_df) <-primary_fields_a
    
    secondary_df <- secondary_request$feeds
    colnames(secondary_df) <- secondary_fields_a
    
    
  } else {
    
    # assign B field names
    primary_df <- primary_request$feeds
    colnames(primary_df) <-primary_fields_b
    
    secondary_df <- secondary_request$feeds
    colnames(secondary_df) <- secondary_fields_b
    
  }

  df <- full_join(primary_df, secondary_df)
  
  
  return(df)
  
  
}


# for testing purposes
row <- pa_sf[1,]
df <- thingspeak_collect(row, "2016-01-01", "2018-05-15")

# apply our read function across each row of our pa_sf df
apply(pa_sf
      ,MARGIN = 1 # applies over rows
      ,FUN = thingspeak_collect
      )

